# 从人脸图像中检测学者的专业

> 原文:[https://dev . to/n1 try/detecting-academics-major-from-face-images-280 I](https://dev.to/n1try/detecting-academics-major-from-facial-images-280i)

# [](#the-idea)想法

几个月前我读了一篇题为[“深度神经网络在从面部图像中检测性取向方面比人类更准确”](https://www.gsb.stanford.edu/faculty-research/publications/deep-neural-networks-are-more-accurate-humans-detecting-sexual)的论文，这引起了很多[的争议](https://news.ycombinator.com/item?id=15198997)。虽然我不想评论这篇论文的方法论和质量(已经有人评论过了，比如在[的一篇杰瑞米·霍华德](https://www.fast.ai/2017/09/13/kosinski/)的文章中)，但我发现它非常有趣和鼓舞人心。简而言之，研究人员从约会网站上收集了面部照片，并建立了一个机器学习模型来对人们的性取向进行分类，并用他们的方法达到了令人印象深刻的准确性。

[这篇客座博文](https://scatter.wordpress.com/2017/09/10/guest-post-artificial-intelligence-discovers-gayface-sigh/)将结果总结为:

> 人工智能不能辨别你是否是同性恋…但它能辨别你是否是一个刻板印象。

事实上，我们经常看到看起来很老套的人。我试图想出更多这样的场景，并得出结论，这种现象经常出现的另一个环境是大学校园。所以你经常在校园里走来走去，看到学生，他们看起来就像一个法律学生，一个计算机科学呆子，一个运动员，等等。有时候我很好奇，几乎想问他们我的假设是否正确。

在阅读了上述论文后，我想知道一些机器学习模型是否能够量化这些潜在的假设，并找出一个刻板印象的学生的职业或专业。

虽然我在机器学习方面只有一点点基础知识，特别是在使用深度神经网络的图像分类方面，但我把它作为一个个人挑战来建立一个分类器，根据他们的面部图像检测学者的专业。

# [](#disclaimer)免责声明

请不要把这篇文章看得太重。我不是机器学习专家，也不是专业科学家。我的方法或实现中可能会有一些错误。然而，我很想听听你的想法和反馈。

# [](#approach)接近

我的第一个(也是最后一个)方法是 **(1。)收集学生**或其他学者 **(2)的脸部照片。)根据他们的专业，用一个小的、有限的课程集给他们贴上标签**，最终成为 **(3。)拟合一个卷积神经网络(CNN)** 作为分类器。我想到了一些研究领域，这些领域的学生可能看起来有点老套，于是我想出了四个类别:

1.  计算机科学
2.  经济学(~ econ)
3.  (德语)语言学(~德语)
4.  机械工程(~机械)

请注意，这并不意味着以任何方式冒犯！(我自己就是个计算机科学呆子😉).

# [](#getting-the-data)获取数据

第一个先决条件是训练数据——像往常一样，当进行机器学习时。由于我的目标是训练一个卷积神经网络(CNN)，所以应该有很多数据，最好是这样。

虽然在我的校园里走一圈，问学生他们的专业和他们的照片是一种有趣的方式，但我可能不会得到很多数据。相反，我决定从大学网站上抓取图片。几乎每个大学的每个系都有一个叫做“[职员](http://dbis.ipd.kit.edu/english/722.php)”、“人物”、“研究人员”之类的网页在他们的网站上。虽然这些不是特别的学生名单，而是教授、研究助理和博士候选人的名单，但我认为这些照片应该仍然足以作为训练数据。

我用 Python 和 [Selenium WebDriver](https://www.seleniumhq.org/) 写了一堆**爬虫脚本**爬行 **57** 不同的网站，包括以下大学的各个院系的网站:

*   卡尔斯鲁厄理工学院
*   慕尼黑大学
*   慕尼黑大学
*   维尔茨堡大学
*   锡根大学
*   拜罗伊特大学
*   弗赖堡大学
*   海德堡大学
*   埃尔兰根大学
*   班贝格大学
*   曼海姆大学

经过一点手动数据清理(删除没有人脸的图片，旋转图片，...)，最终我得到了总共 **1369** 张来自四个不同类的标签图片。虽然这对于训练 CNN 来说不是很多数据，但我还是决定试一试。

## [](#examples)例子

### [](#images)图像

**抓取后包含所有 raw 图片的文件夹节选:**
[![Excerpt from all crawled raw images](../Images/954199a6f0fdbddb667476175d5ece99.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--Vqrr8HDI--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://muetsch.io/images/academic_faces1.png) 
(如果你在这些图片中的某一张，想要删除，请联系我。)

### [](#labels)标签

**来自`index.csv`的摘录，包含每个图像的标签和元数据:**

```
id,category,image_url,name
c35464fd,mechanical,http://www.fast.kit.edu/lff/1011_1105.php,Prof. Dr. rer. nat. Frank Gauterin
a73d11a7,cs,http://h2t.anthropomatik.kit.edu/21_1784.php,Kevin Liang
97e230ff,econ,http://marketing.iism.kit.edu/21_371.php,Dr. Hanna Schumacher
cde71a5c,german,https://www.germanistik.uni-muenchen.de/personal/ndl/mitarbeiter/bach/index.html,Dr. Oliver Bach 
```

<svg width="20px" height="20px" viewBox="0 0 24 24" class="highlight-action crayons-icon highlight-action--fullscreen-on"><title>Enter fullscreen mode</title></svg> <svg width="20px" height="20px" viewBox="0 0 24 24" class="highlight-action crayons-icon highlight-action--fullscreen-off"><title>Exit fullscreen mode</title></svg>

# [](#preprocessing-the-data)预处理数据

在图像可以用作学习算法的训练数据之前，需要进行一些预处理。我主要做了两个主要的预处理步骤。

1.  **裁剪**图像到人脸——如你所见，图片是从不同角度拍摄的，有些包含大量背景，有些没有居中等。为了获得更好的训练数据，图片必须只被裁剪到脸部，而不是其他部分。
2.  **缩放** -所有图片都有不同的分辨率，但最终需要完全相同的大小，以便用作神经网络的输入。

为了实现这两个预处理步骤，我使用了一个很棒的、小巧的、基于 OpenCV 的 Python 工具，名为 [autocrop](https://github.com/leblancfg/autocrop) ，命令如下:

`autocrop -i raw -o preprocessed -w 128 -H 128 > autocrop.log`。

这将检测`raw`文件夹中每张图片中的人脸，将图片裁剪为该人脸，将生成的图像重新缩放为 128 x 128 像素，并保存到`preprocessed`文件夹中。当然，在某些图片中，算法无法检测到人脸。这些被记录到 stdout 并保存到`autocrop.log`。

此外，我编写了一个脚本来解析`autocrop.log`以获得失败的图像，并随后将图像分成*训练* (70 %)、*测试* (20 %)和*验证* (10 %)，并将它们复制到一个与 [Keras ImageDataGenerator](https://keras.io/preprocessing/image/) 读取训练数据所需的格式兼容的文件夹结构中。

```
- raw
    - index.csv
    - c35464fd.jpg
    - a73d11a7.jpg
    - ...
- preprocessed 
    - train
        - cs
            - a73d11a7.jpg
            - ...
        - econ
            - 97e230ff.jpg
            - ...
        - german
            - cde71a5c.jpg
            - ...
        - mechanical
            - c35464fd.jpg
            - ...
    - test
        - cs
            - ...
        - ...
    - validation
        - cs
            - ...
        - ... 
```

<svg width="20px" height="20px" viewBox="0 0 24 24" class="highlight-action crayons-icon highlight-action--fullscreen-on"><title>Enter fullscreen mode</title></svg> <svg width="20px" height="20px" viewBox="0 0 24 24" class="highlight-action crayons-icon highlight-action--fullscreen-off"><title>Exit fullscreen mode</title></svg>

# [](#building-a-model)建立模型

## [](#approach-1-simple-custom-cnn)方法一:简单，自定义 CNN

**代码**

*   [custom_model.ipynb](https://gist.github.com/n1try/78bf6d7929e4facd199ad0ffea0b3ad9)

我决定从简单的开始，看看是否能从数据中学到什么。我在 Keras 中定义了以下简单的 CNN 架构:

```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 62, 62, 32)        320       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 29, 29, 32)        9248      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 12, 12, 32)        9248      
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 6, 6, 32)          0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 1152)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                73792     
_________________________________________________________________
dropout_1 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 4)                 260       
=================================================================
Total params: 92,868
Trainable params: 92,868
Non-trainable params: 0 
```

<svg width="20px" height="20px" viewBox="0 0 24 24" class="highlight-action crayons-icon highlight-action--fullscreen-on"><title>Enter fullscreen mode</title></svg> <svg width="20px" height="20px" viewBox="0 0 24 24" class="highlight-action crayons-icon highlight-action--fullscreen-off"><title>Exit fullscreen mode</title></svg>

我用了 Keras 的 [ImageDataGenerator](https://keras.io/preprocessing/image/) (很棒的工具！)要将图像读入 NumPy 数组，请将它们重新缩放为`(64, 63, 3)` (64 x 64 像素，RGB)的形状，并使用旋转、缩放、水平翻转等变换来执行一些数据扩充。来放大我的训练数据，并希望建立更健壮、更少过度拟合的模型。

我让模型训练 **100 个时期**，使用带有默认参数的 **Adam 优化器**和**分类交叉熵损失**，一个小批量的 **32** 和**3 倍增加**(使用转换将训练数据放大三倍)。

### [](#results-571-accuracy)结果(准确率 57.1 %)

在 74 个时期后，达到 0.66 的最大**验证精度。**测试精度**竟然是 **0.571** 。考虑到一个非常简单的模型是用不到 1000 个训练样本完全从零开始训练的，我对这个结果印象深刻。这意味着平均而言，该模型预测的正确率超过了每两个学生中的一个。一个正确分类**的**先验概率**是 0.25** ，所以模型肯定学到了至少一些东西。**

## [](#approach-2-finetuning-vggface)方法二:微调 VGGFace

**代码**

*   [vgg faces _ 瓶颈 _ 模型. ipynb](https://gist.github.com/n1try/a079dcb27d921d58323c9574152b2c2d)
*   [vgg faces _ fine tuned _ model . ipynb](https://gist.github.com/n1try/c3b9e9401f178807c91ad890a6c67e18)

作为一个简单的、自定义的 CNN 模型(从头开始训练)的替代方案，我想遵循微调现有预训练模型权重的通用方法。这种方法的基本思想是不“重新发明轮子”，而是利用以前已经学到的知识，并且仅仅稍微调整“知识”(以重量的形式)来解决某个问题。图像中的潜在特征，即学习算法之前已经从大量训练数据中提取的特征，可以被充分利用。[“使用 Keras 中预训练模型的图像分类”](https://www.learnopencv.com/keras-tutorial-using-pre-trained-imagenet-models/)极好地概述了**微调**如何工作，以及它与**转移学习**和定制模型有何不同。期望我给定的分类问题可以用更少的数据更准确地解决。

我决定以一个在 [**VGGFace**](http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/) 上训练的 **VGG16** 模型架构为基础(使用 [keras-vggface](https://github.com/rcmalli/keras-vggface) 实现)并按照[这个指南](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)进行微调。VGGFace 是牛津大学发布的数据集，包含超过 330 万张人脸图像。因此，我希望它能够提取非常健壮的面部特征，并且非常适合面部分类。

### [](#step-1-transferlearning-to-initialize-weights)第一步:转移-学习初始化权重

我的实现包括两步，因为[建议](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)

> 为了执行微调，所有层都应该从正确训练的权重开始。

在第一步中，迁移学习用于为一组新添加的自定义全连接分类层找到合适的权重。这些在后面的步骤 2 中用作初始权重。为了执行这种初始化，预先训练的 VGGFace 模型，在最终分类层被切断的情况下，被用于为每个图像提取 128 个*瓶颈特征*。随后，另一个由完全连接的层组成的微小模型根据这些特征进行训练，以执行最终的分类。权重被保存到一个文件中，并在步骤 2 中再次加载。

模型架构是这样的:

```
________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 128)               65664     
_________________________________________________________________
dropout_1 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 4)                 516       
=================================================================
Total params: 66,180
Trainable params: 66,180
Non-trainable params: 0 
```

<svg width="20px" height="20px" viewBox="0 0 24 24" class="highlight-action crayons-icon highlight-action--fullscreen-on"><title>Enter fullscreen mode</title></svg> <svg width="20px" height="20px" viewBox="0 0 24 24" class="highlight-action crayons-icon highlight-action--fullscreen-off"><title>Exit fullscreen mode</title></svg>

### [](#step-2-finetuning)第二步:微调

在第二步中，预训练的 VGGFace 模型(冻结前 n - 3 层)与第一步中预训练的顶层结合使用，以微调我们特定分类任务的权重。它将(128，128，3)形状的张量(128 x 128 像素，RGB)作为输入，并预测我们四个目标类的概率。

组合模型的架构是这样的:

```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
vggface_vgg16 (Model)        (None, 512)               14714688  
_________________________________________________________________
top (Sequential)             (None, 4)                 66180     
=================================================================
Total params: 14,780,868
Trainable params: 2,425,988
Non-trainable params: 12,354,880 
```

<svg width="20px" height="20px" viewBox="0 0 24 24" class="highlight-action crayons-icon highlight-action--fullscreen-on"><title>Enter fullscreen mode</title></svg> <svg width="20px" height="20px" viewBox="0 0 24 24" class="highlight-action crayons-icon highlight-action--fullscreen-off"><title>Exit fullscreen mode</title></svg>

`top`是步骤 1 中描述的模型，`vggface_vgg16`是 VGG16 模型，看起来像这样:

```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         (None, 128, 128, 3)       0         
_________________________________________________________________
conv1_1 (Conv2D)             (None, 128, 128, 64)      1792      
_________________________________________________________________
conv1_2 (Conv2D)             (None, 128, 128, 64)      36928     
_________________________________________________________________
pool1 (MaxPooling2D)         (None, 64, 64, 64)        0         
_________________________________________________________________
conv2_1 (Conv2D)             (None, 64, 64, 128)       73856     
_________________________________________________________________
conv2_2 (Conv2D)             (None, 64, 64, 128)       147584    
_________________________________________________________________
pool2 (MaxPooling2D)         (None, 32, 32, 128)       0         
_________________________________________________________________
conv3_1 (Conv2D)             (None, 32, 32, 256)       295168    
_________________________________________________________________
conv3_2 (Conv2D)             (None, 32, 32, 256)       590080    
_________________________________________________________________
conv3_3 (Conv2D)             (None, 32, 32, 256)       590080    
_________________________________________________________________
pool3 (MaxPooling2D)         (None, 16, 16, 256)       0         
_________________________________________________________________
conv4_1 (Conv2D)             (None, 16, 16, 512)       1180160   
_________________________________________________________________
conv4_2 (Conv2D)             (None, 16, 16, 512)       2359808   
_________________________________________________________________
conv4_3 (Conv2D)             (None, 16, 16, 512)       2359808   
_________________________________________________________________
pool4 (MaxPooling2D)         (None, 8, 8, 512)         0         
_________________________________________________________________
conv5_1 (Conv2D)             (None, 8, 8, 512)         2359808   
_________________________________________________________________
conv5_2 (Conv2D)             (None, 8, 8, 512)         2359808   
_________________________________________________________________
conv5_3 (Conv2D)             (None, 8, 8, 512)         2359808   
_________________________________________________________________
pool5 (MaxPooling2D)         (None, 4, 4, 512)         0         
_________________________________________________________________
global_max_pooling2d_3 (Glob (None, 512)               0         
=================================================================
Total params: 14,714,688
Trainable params: 2,359,808
Non-trainable params: 12,354,880 
```

<svg width="20px" height="20px" viewBox="0 0 24 24" class="highlight-action crayons-icon highlight-action--fullscreen-on"><title>Enter fullscreen mode</title></svg> <svg width="20px" height="20px" viewBox="0 0 24 24" class="highlight-action crayons-icon highlight-action--fullscreen-off"><title>Exit fullscreen mode</title></svg>

我再次使用 Keras *ImageDataGenerator* 加载数据，增加(3 倍)和调整大小。正如[推荐的](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)，*随机梯度下降*与小学习率(10^-4)一起使用，以仔细调整权重。该模型在**批 32 个图像**上被训练 **100 个时期**，并且再次使用**分类交叉熵**作为损失函数。

### [](#results-546-accuracy)结果(准确率 54.6 %)

经过 38 个时期后，最大验证精度达到 0.64 。**测试精度**结果是 **0.546** ，这是一个相当令人失望的结果，考虑到即使是我们简单的定制 CNN 模型也达到了更高的精度。可能模型的复杂度对于少量的训练数据来说太高了？

# [](#inspecting-the-model)检查模型

为了更好地了解模型的表现，我简要地考察了它的几个标准。这是我发现的一个简短总结。

## [](#code)代码

*   [inspection.ipynb](https://gist.github.com/n1try/404befcfb2eef4b59398f3c8590ce692)

## [](#class-distribution)阶级分布

我首先看的是班级分布。在我们的数据中，这四个研究主题是如何表示的？这个模型预测了什么？

|  | 铯 | 经济 | 德国的 | 机械的 |
| --- | --- | --- | --- | --- |
| *真实* | 0.2510 | 0.2808 | 0.2127 | 0.2553 |
| *pred* | 0.2595 | 0.2936 | 0.1361 | 0.3106 |

显然，这个模型忽略了德国语言学家的阶层。这也是我们拥有最少训练数据的类别。也许我应该收集更多。

## [](#examples-of-false-classifications)错误分类的例子

我想知道这个模型哪里做错了，哪里做对了。因此，我看了看最上面的五个 **(1)假阴性**、 **(2)假阳性**和 **(3)真阳性**。

以下是经济类*的摘录:*

[![](../Images/57fa3c26811fd25c3a54608652b64aea.png)T2】](https://res.cloudinary.com/practicaldev/image/fetch/s--ZvZ2oI74--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://muetsch.io/images/academic_faces2.png)

最上面一行显示的是经济学家的例子，模型没有识别出这些经济学家。中间一行描述了模型“认为”的经济学家的样子，但他们实际上是不同专业的学生/研究人员。
最后，底行显示了良好匹配的示例，即模型对其实际类别具有非常高的置信度的人。

同样，如果你在这些照片中，并且想被删除，请联系我。

## [](#confusion-matrix)混淆矩阵

为了查看模型不确定的职业，我计算了混淆矩阵。

```
array([[12.76595745,  5.95744681,  0\.        ,  6.38297872],
       [ 3.40425532, 12.76595745,  3.82978723,  8.08510638],
       [ 3.82978723,  5.53191489,  8.5106383 ,  3.40425532],
       [ 5.95744681,  5.10638298,  1.27659574, 13.19148936]]) 
```

<svg width="20px" height="20px" viewBox="0 0 24 24" class="highlight-action crayons-icon highlight-action--fullscreen-on"><title>Enter fullscreen mode</title></svg> <svg width="20px" height="20px" viewBox="0 0 24 24" class="highlight-action crayons-icon highlight-action--fullscreen-off"><title>Exit fullscreen mode</title></svg>

[![](../Images/71ff4d15107023e35b28f09d4ba573d4.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--htz4jG4F--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://muetsch.io/images/academic_faces3.png) 
**图例:**

*   0 =计算机科学，1 =经济，2 =德语，3 =机械
*   更亮的颜色~更高的价值

我们可以从混淆矩阵中读到的是，例如，该模型往往将经济学家归类为机械工程师。

# [](#conclusion)结论

首先，这不是一个科学研究，而是我的一个小爱好项目。此外，它在现实世界中没有太大的重要性，因为人们可能很少想把学生分成四类。

虽然结果并不壮观，但我仍然很高兴，至少我的模型比随机猜测要好很多。假设四个类别的准确率为 57 % ,你可以肯定地说，从某种程度上来说，仅从长相刻板的人的面部图像中，就可以了解他们的学习专业。当然，这只在有限的上下文和一组限制下成立，但对我来说这仍然是一个有趣的见解。

此外，我非常肯定，该模型仍有很大的改进空间，可以产生更好的性能。这些可能包括:

*   来自更广泛来源的更多培训数据
*   更彻底的预处理(例如过滤掉秘书的图像)
*   不同的模型架构
*   超参数调谐
*   手动特征工程
*   ...

请让我知道你对这个项目的看法。我很想得到一些反馈！

-

最初发布于 [muetsch.io](https://muetsch.io/detecting-academics-major-from-facial-images.html) 。