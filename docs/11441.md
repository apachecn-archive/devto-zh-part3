# A/B 测试的工程问题

> 原文:[https://dev . to/dgieselaar/the-engineering-problem-of-ab-testing-ok1](https://dev.to/dgieselaar/the-engineering-problem-of-ab-testing-ok1)

在过去的两年里，我在一个 A/B 测试是主要推广机制的环境中工作——对我来说是一种全新的体验。最近，尼古拉斯·加拉格尔在推特上发布了 37 个词，相当准确地总结了我至少一半的 A/B 测试经验，并促使我写了一篇关于它的文章。

> ![](../Images/87e679709bc74ceba8fac5b86ce176e3.png)尼古拉斯@内科拉斯![](../Images/4d9c44713c216584b3d48ff3455cbb68.png)想象一下，在你的个人生活中，你从未采取(或被允许采取)任何行动，除非你能首先量化或衡量它的影响。现在你明白了影响科技行业的病理之一01:35AM-2019 年 2 月 15 日[![Twitter reply action](../Images/269095962147c28351274afdd5486a48.png)](https://twitter.com/intent/tweet?in_reply_to=1096221537537933312)[![Twitter retweet action](../Images/771160ecf06ae3d4d7a7815c29c819c2.png)](https://twitter.com/intent/retweet?tweet_id=1096221537537933312)243[![Twitter like action](../Images/c077611ab2a5e0b4cd0c826ee7ae1e48.png)](https://twitter.com/intent/like?tweet_id=1096221537537933312)988

## A/B 测试的承诺

A/B 测试或多变量测试是一种机制，用于比较同一功能或页面的两个(或多个)版本，并比较这些版本的统计数据，以查看哪个版本的性能更好。理想情况下，这将导致更多(数据)明智的决策，并实现快速反馈循环和持续改进。经典的例子(也是我第一次听说 A/B 测试): [Google 曾经测试过 40 种深浅的蓝色，看看哪种颜色让用户转化的最多](https://iterativepath.wordpress.com/2012/10/29/testing-40-shades-of-blue-ab-testing/)。

在高层次上，它是这样工作的(或者，它应该如何工作):

*   形成了一个关于用户行为的假设(通常基于心理学理论，比如[害怕错过](https://en.wikipedia.org/wiki/Fear_of_missing_out))。为了测试这个假设，我们创建了一个*实验*，在这里我们测试当前的实现(通常称为“控制”)，以及一个新实现的一个或多个变体。在测试开始之前，需要决定实验将评估哪些指标。
*   用户访问网站或应用程序，特别是您正在测试的功能。
*   然后它们被*分桶*，这意味着它们被分配给你正在测试的实验的一个变种。用户在哪个桶中着陆，决定了他们可以看到和使用哪个版本的实验。在整个实验过程中，它们都呆在这个桶里。
*   用户行为事件被记录并存储用于统计分析。通常，实验至少会持续一周或更长时间。一旦完成，就对结果进行分析，并且在大多数情况下，向所有用户展示实验的最佳表现变体。

## [](#setting-up-an-experiment)设置一个实验

现在，假设您想要实现一个 A/B 测试。你需要什么？以下是您可以在任何解决方案中找到的一些关键功能:

*   **分桶**:用户需要分布在桶中。本质上，它是一个`Math.random() > 0.5`。通常，这种分割是 50/50(或者在变体之间平均分割)，但是如果你运行一个风险更大的实验，你可能会做类似 80/10/10 的分割。您还可以使用受众或环境目标来确定用户是否有资格参加实验。远程配置:这有助于通过与代码分离的*配置来开关实验。在大多数情况下，部署新版本的应用程序是一个繁琐的过程，管理实验的人通常不是工程师。理想情况下，产品经理/营销人员可以通过一个独立的过程来管理配置，这个过程比推出一个新版本更便于他们使用。至少，你会想要管理哪些实验被启用(因为，你知道，事情*打破*)，以及像开始和结束日期，或目标。*
*   **跟踪**:在我们的实验中，你还需要获得用户行为的数据，这样你就可以评估哪种变体表现得更好。我们的用户在页面上花了多少时间？他们会转向漏斗中的下一步吗？他们在篮子里放了多少购物物品？您需要挂钩到这些事件，记录它并将其保存在一个中心位置…
*   **分析**:一旦你有了数据，实验结束，你需要分析结果。一些基本的数字:在变体 B 或 C 中有多少用户？根据您选择的指标，他们的表现如何？测试的统计意义是什么？也许你想根据设备类型或受众数据进行细分。你可以从数据库中导入统计数据并使用基本的 excel 表格，或者你可以使用像 Google Analytics 这样的工具，例如允许你查询序列，这在分析用户行为时非常有用。可视化编辑器:Optimizely 这样的工具提供了一个可视化编辑器，允许你点击自己的方式来设计一个新的实验。如果你无法直接接触到工程团队，这非常有用(如果你*有*这样的团队，可能会有更好的选择)。

## [](#implementation-approaches)实现途径

据我所知(诚然有限)，至少有五种方法可以实现 A/B 测试:

*   Canary releases :如果你想测试你的网站的一个新版本，你可以为你想测试的每个版本部署一个新版本(也许是一个特性分支)，然后将你的用户子集(带有粘性会话)路由到这个新部署。为了能够使用它，您必须有一个管理良好的基础设施和发布管道，特别是当您想要并行运行多个测试，并且需要许多不同的部署和随之而来的路由复杂性时。很可能你也需要相当大的流量。不过，好处似乎很明显。例如，任何失败的实验都不会引入技术债务(代码永远不会落到主服务器上，部署只是被删除)。另一个好处是，这使得用户一次只能参与一个实验；多重实验带来了技术挑战和实验相互影响的不确定性。

*   **分割网址**:谷歌历史上推荐的防止搜索引擎优化问题的方法，你可以使用网址将用户导向不同的实验。举个例子:`/amazing-feature/test-123/b`。这种方法的好处是，当你尝试不同的设计时，你不会对你的域名上给定的 URL 的 SEO 值产生负面影响。

*   **服务器端**:当一个页面被请求时，用户被存储在服务器上。然后设置一个 cookie 来确保用户被“困”在这个桶中，它用于呈现用户正在进行的任何实验的界面。你几乎可以做任何你想做的事情:A/B 测试、多元测试、特征切换、平行实验:一切都取决于你。对于用户来说，这是最佳选择之一，因为对性能的影响可以忽略不计。然而，因为你使用 cookies，CDN 的好处是有限的。Cookies 在请求中引入了变化(特别是如果用户可以输入多个实验)，并且它将导致缓存未命中，使您没有 CDN 的保护。

*   **客户端**:如果你无法访问服务器，或者你想拥有最大的灵活性，客户端 A/B 测试也是一个选择。在这种情况下，要么不呈现任何界面，要么呈现原始界面，并且在这种情况发生时或稍早时，实验被激活，并且基于用户所在的任何变体来增强界面。当您无法访问工程团队，并且使用外部工具来运行实验时，这种选择通常是有意义的。然而，就性能而言，这通常是最差的选择。作为一个例子，让我们看看客户端优化是如何实现的:你嵌入了一个`blocking`脚本，它迫使浏览器等待在屏幕上显示任何东西，直到这个脚本被下载、编译和执行。此外，浏览器将降低所有其他资源的优先级(可能需要这些资源来逐步增强您的网站),以便尽可能快地加载阻止脚本。最重要的是，如果您不自托管脚本，浏览器必须预连接到另一个源，并且它只能缓存几分钟(也就是说，如果您希望能够尽快关闭破坏转换的实验)。通过对移动连接的综合测试，我发现关键事件的延迟在 1-2 秒之间。慎用！

*   **在边缘**:如果你的网站前面有 CDN，可以借助边缘工作者的力量运行实验。我将向 [Scott Jehl 了解关于它的细节](https://www.filamentgroup.com/lab/servers-workers.html)，但它的要点是，你的服务器呈现你的界面的所有变化，你的 CDN 缓存这个响应，然后当用户加载你的网站时，缓存的响应被提供，在边缘工作者删除不适用于请求它的用户的 HTML 之后。如果您关心性能，这是一个非常有前途的方法，因为您可以获得 CDN 的好处，而不会对浏览器性能产生任何影响。

## [](#the-reality-of-ab-testing)现实的 A/B 测试

原来，A/B 测试是很难的。它可能非常有价值，我认为你应该用你的底线来衡量它。然而，这并不是灵丹妙药，你必须根据你所在的公司类型(或希望成为的公司类型)来定制你的方法。以下是我在一家每天大约有 5 万到 10 万用户的中型公司学到的东西:

### [](#isolate-experiments-as-much-as-possible)尽可能隔离实验

在我现在的雇主那里，我们正在并行地实现实验，并且无论是否会被使用(基本上，一个特性切换)，实现总是在一经验证就投入生产。这主要是由于我们的技术选择:我们有一个服务器端渲染，重新水合，单页应用程序，这使得很难使用金丝雀策略(因为你永远不会回到路由器或负载平衡器)。除此之外，由于缺乏流量，我们负担不起每个用户跨平台进行一次实验的奢侈。实际上，这意味着*实验有副作用*。这里有两个问题。

首先，并发实现的实验使得任何合理的端到端测试覆盖预期都是不可能的:即使像 10 个 A/B 测试这样的少量测试也会产生 100 种应用程序变体。为了测试所有这些不同的变化，我们的测试需要 250 个小时，而不是 15 分钟。因此，我们在测试期间禁用所有实验。这意味着任何实验都可能——并且最终会——破坏关键(和非关键)用户功能。此外，除了我前面提到的缓存问题之外，它还使得从您的错误报告系统中可靠地重现 bug 变得更加困难(重现本来就够困难的了！).

其次，在用户的旅程中运行多个实验会导致测试结果的不确定性。假设你有一个关于产品页面的实验，和一个关于搜索的实验。如果搜索实验对你发送到产品页面的流量类型有很大影响，那么产品实验的结果就会有偏差。

我能想到的最好的隔离策略是金丝雀发布和特色分支。在我狂野的清醒梦中，事情是这样的:当你开始一个实验时，你创建了一个分支，它包含了一个变体的变化。您打开一个 pull 请求，然后部署一个包含该变更的测试环境。通过审查后，它将被部署到生产环境中，路由器配置将被更新，以便向您想要测试的变体发送一定量的流量。您必须考虑预期的使用情况、一般流量和测试的预期持续时间，以确定什么样的流量分流是有意义的。假设您估计一周 20%的流量就足够了，那么通常会排除 80%的流量用于测试，并将剩余的 20%平均分配给运行您网站当前版本的实例和运行该实验变体的版本。

[![A diagram showing an A/B testing architecture<br>
](../Images/560a61299144e7a3ebd61b497df511a9.png)T2】](https://res.cloudinary.com/practicaldev/image/fetch/s--kePKiyBu--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn-images-1.medium.com/max/2000/1%2A4j4wZDdakosnMPfvhsmwOw.png)

我可以想象协调这需要大量的工程工作，尤其是当你想自动打开和关闭实验，或者当你想使用更先进的目标。你必须有足够的流量，在这一点上，你会看到将你的网站分成更小的可部署单元的好处。例如，你可以考虑将你的前端拆分成[微前端](https://www.youtube.com/watch?v=BuRB3djraeM)。

如果你不能恰当地隔离实验，你可以试着接受生活中不是所有的问题都是或者应该是可以解决的。如果你是一个控制狂(像我一样)，你可能会考虑互斥实验——这意味着在实验 X 中的用户不能同时在实验 Y 中。这将有助于消除行为副作用。如果您需要更多的测试信心，您可以选择较低级别的测试，比如单元或组件测试。或者，你可以处理 250 小时以上的管道，无论你的船漂浮。

### [](#stick-to-high-standards)坚持高标准

围绕 A/B 测试的一个经常重复的口头禅是“这只是一个测试，我们稍后会修复它”。这里的想法是，你建立一个 MVP 并衡量兴趣，如果有的话，你建立一个设计更好、实现更好的版本作为最终版本。实际上，我并没有看到这种效果，大概有两个原因:第一个原因是，修理东西的动力在产品出厂后就消失了。这适用于所有各方:工程、设计和产品。它已经被证明是一种提升，花时间重新设计或重新分解会感觉没有必要。而那些*觉得*不需要的事情——即使需要——也不会发生，尤其是在产品工程这个高压锅里。第二个原因是，重新实施一个实验，甚至重新设计它，都可能对任何先前假设的提升产生影响。为了绝对确定，你必须运行*另一个*实验，现在已经可以生产了。没人有时间做这个，头儿。事情是这样的:需要为实验的实现走捷径的环境类型也不太可能分配时间来重构或/和重新运行一个成功的实验。

会发生什么？你积累了技术债务。通常没有明确的范围和定量的描述。很难给债务加上一个数字，也很难让它得到解决。债务会慢慢向你袭来，直到最后，每个人都放弃了，拿出了重写的锤子。(我会在这一点上引用尼古拉斯的推文)。

不同的标准不仅不明智，而且令人困惑。让工程师们遵循一个标准已经够难了，但是两个呢？*不可能*。做好准备，迎接代码审查中无休止的反复。(就个人而言——好像以前的宣言不仅仅如此——更低的标准也同样令人沮丧。但也许这只是我的想法。)

### [](#aim-for-impact)瞄准冲击

VWO，一个转换优化平台，估计大约七分之一的实验失败。在 CRO 世界，人们常说，失败没关系，只要你能从中吸取教训。这里的假设是，知道什么不起作用，和知道什么起作用一样有价值。

然而，这并不意味着你应该开始尝试那些仅仅是猜测，和/或可以通过常识、经验或定性研究发现的东西。这些选项中的每一个都比扔掉每年 10 万名设计师或开发人员的 85%以上的能力更便宜——特别是如果你考虑到流失率，如果你的员工觉得他们的所有贡献都毫无意义，这将不可避免地发生。

你如何保持高昂的士气，让贡献者感到他们受到重视？当然，认同和强调学习是有帮助的。但对我来说，大赌注是最鼓舞人心的。它们让我能够充分利用我的经验和技能来有所作为。现在，什么是大赌注取决于你是什么类型的公司，但我不认为重新定位或复制实验属于这一类。将标准设置得太低的一个很好的迹象是许多非决定性的实验(或者必须运行很长时间才有意义的实验)。如果是这样的话，你就下了太多的小赌注。

## 现在，回到那条推文…

诚然，我使用 Nicolas 的 tweet 主要是为了对一个相当无聊的话题进行一个漂亮、吝啬的介绍，但它承载了一个强有力的真理:数据，或者对数据的需求，通常会导致惰性。数据没有所有的答案。它不能取代愿景，也不是战略。

定义一个愿景，然后使用 A/B 测试来验证你在实现这个愿景的过程中取得的进展。而不是相反。