# 扩展到一百万 RPS

> 原文:[https://dev.to/karsens/scaling-to-one-million-rps-19il](https://dev.to/karsens/scaling-to-one-million-rps-19il)

[![scale](../Images/8d444c908b54ef0a80eeec5165f370b3.png)T2】](https://res.cloudinary.com/practicaldev/image/fetch/s--kWgyr3Zc--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/mcs1cny71liy2bx2306p.jpg)

这篇[福布斯文章](https://www.forbes.com/sites/reuvencohen/2013/11/26/google-shows-how-to-scale-apps-from-zero-to-one-million-requests-per-second-for-10/#7de604137ad9)和它的[原帖](https://cloudplatform.googleblog.com/2013/11/compute-engine-load-balancing-hits-1-million-requests-per-second.html)描述了一个家伙如何设法用一个负载平衡器和 10 美元在一段时间内达到 100 万 RPS。这篇文章还包含了复制实验的要点，这很酷。这个[脸书博客](https://www.facebook.com/notes/facebook-engineering/scaling-facebook-to-500-million-users-and-beyond/409881258919/)展示了他们如何在 2010 年扩大到 5 亿用户。在这篇也是 2010 年的文章中，脸书给出了关于他们的统计数据和扩展策略的见解。诸如此类的故事让我开始思考 [Communify](https://communify.cc/) 的未来规模。脸书有 5 亿用户，每秒有 1300 万次请求。现在，有 40 亿用户，他们可能每秒处理 1 亿个请求。我想知道适当的缩放怎么会在那里结束。

# [](#my-geo-scaling-plan-scaling-without-bottlenecks)我的地理扩展计划:无瓶颈扩展

由于我们现在有全球社区，从一台服务器上获取一个用户的数据几乎是不可能的。因此，负载平衡器可能需要了解哪些社区位于何处。然后，负载平衡器可以根据请求将查询发送到正确的服务器。

如果不是这种情况，那就很容易了，就像我在[上一篇关于地理缩放的文章](https://medium.com/leckr-react-native-graphql-apollo-tutorials/the-benefits-and-drawbacks-of-decentralised-geo-scaling-thinking-of-2019-and-beyond-infinite-9faa5ad465c8)中描述的那样。但是现在我们也有了全球社区，我们有一些技巧要做。这是我想到的:

*   **PULL** 每当推送新的提交时，自动将我的 GitHub repo 拉至任何地方。拉，重新启动 pm2，并自动将数据库更改合并到数据库。

*   **拆分**每当负载过高时，自动将一台服务器拆分为两台服务器。社区上的碎片。服务器被一分为二，所以应该在地图上画一条线(按位置分组),这样所有成员就可以根据位置一分为二。存在一种边缘情况，其中用户是两个集群中团体的成员。在这种情况下，将用户放在两台服务器上。这没什么大不了的。新服务器应该让负载平衡器知道它的存在，并公开它的社区。

*   **合并**每当负载过低时，自动将一台服务器与另一台服务器合并。因为我们合并服务器，所以我们不能使用增量 ID，而是需要一个 UUID 或 GUID 来保持表行的唯一性。因为分片/分割是在社区上完成的，所以我们可以将所有表中的所有行合并在一起。但是因为用户可能在两个集群中，所以可能存在双重用户。如果发生这种情况，选择最后更新的用户，因为那是用户最后活动的地方。被删除的服务器应该让负载平衡器知道它被删除了，所以它将从全局服务器列表中被删除。

*   **BALANCE** 在它的前面放置一个负载平衡器，根据 communityid 将每个请求导向正确的服务器。它知道哪台服务器有哪些社区，因为每台服务器都通过`communities(){id}`公开了它们的社区，我们知道每台服务器，因此，每隔一分钟左右，我们就可以从所有服务器中查找所有社区 id。这很轻。然而，这也可以反过来进行。当创建或删除社区时，或者当服务器拆分或合并时，负载平衡器会收到新组合的通知。这将是即时和更便宜的方式。

# [](#double-models)双型号

可以基于社区进行分片的模型，因此只需要存在于一个服务器上，并且只有一个副本:帖子、订阅、角色、社区、频道。

有一些问题的模型:用户、社区用户、位置

一个用户可以同时加入两个位于不同服务器上的社区。有几种可能性可以解决这个问题:

1) **复制/粘贴**当用户将当前社区更新为不同服务器上的社区时，将该用户及其所有社区用户和位置复制到新服务器。然后，这将是用户从中获取数据的单一服务器。CommunitySubs 通过来自用户仍然存在的其他服务器的突变调用(增量)来获得通知增量。当用户改变社区时，应该通知所有服务器，这样如果服务器知道用户，它也知道该用户在哪个社区中...这可能会变得很严重，但不会经常发生。这种策略的副作用是，用户、社区用户和位置在用户不活跃的服务器上可能会过时。然而，所有需要知道的服务器，知道一个用户当前住在哪个服务器上，所有的服务器可以获得关于那个用户的更新以保持更新。例如，每小时或每天。不知道会有用多少。原则上，如果一个用户在另一个社区不活跃，他就不会对这个社区有太大的改变。

2) **用户、社区和位置的全球独立数据库**这可能很好，因为它是始终最新的单一事实来源。然而，缺点是应用程序必须连接多个服务器，并且只有一个全局服务器，这对可用性(风险)不利，并且不能无限扩展。

我觉得这是我必须选择的地方，我觉得方案一是最好的。我还得和专家讨论这个问题。这个想法给我留下了深刻的印象，因为我的应用程序可以基于几个假设无瓶颈地无限扩展，我的应用程序设计可以保证:

1)它不会变得比一个负载平衡器所能处理的还要大(大约 1M rps)
2)一个单独的社区永远不会被分割

我认为，这整个架构非常有趣，也将为我的 [Chat-BaaS 想法](https://karsens.com/chat-baas/)服务。

## [](#from-1m-to-100m-rps)从 1 米到 100 米的 RPS。

1 亿 RPS，4000 万用户，4000 万社区，20000 台服务器..这就是梦想！

在一台平衡良好的服务器上，我不认为会出现任何问题。唯一的问题是，当用户改变社区时，所有知道这个用户的服务器都应该知道这一点，所以服务器必须发送 20.000 个请求！对吗？的确如此。除非它知道用户订阅的所有其他社区都托管在哪个服务器上。负载平衡器知道这一点，对吗？因此，让我们询问负载平衡器，然后让服务器知道这一点！太好了！问题解决了。

另一件事是，我们必须用负载平衡器来平衡流量。一个负载均衡器已经不够用了。100 倍的 RPS 应该意味着 100 倍的负载平衡器，但是因为我们为负载平衡器做了一些额外的工作(告诉服务器哪个社区托管在哪里)，我认为 1000 个负载平衡器会更好，只是为了确定。有了 4000 万个社区，让所有的负载平衡器都知道哪些社区托管在哪里仍然可行吗？

*   如果是的话，问题就简单了。只要有一个“主负载平衡器”,将任何新访问者分配到 1000 个负载平衡器中的一个，并保持在那里。从那里，访问者知道去哪里，因为每个负载平衡器都知道一切。从 4000 万行社区中进行搜索是可行的，但瓶颈可能是，在每个用户改变社区(在不同的服务器上)时，每个负载平衡器都会得到通知。问题是:对于 40 亿用户，这种情况每秒发生多少次？如果这个比 5.000 多，瓶颈太小，装不下。我们必须找到另一个解决方案。

*   如果没有，事情就复杂了。将社区分散到所有负载平衡器上可能是一种选择，但这是一个复杂的问题，我必须仔细考虑。我们下次再谈吧！

最初发布于[我的网站](https://karsens.com/scaling/)