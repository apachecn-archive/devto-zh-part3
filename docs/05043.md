# 人工智能革命:深度学习和未来？

> 原文:[https://dev . to/fish root/the-ai-revolution-deep-learning-and-what-s-next-115 l](https://dev.to/fishroot/the-ai-revolution-deep-learning-and-what-s-next-115l)

当谈论“人工智能革命”时，很难缩小一个共同点。这不仅是因为科幻小说没有为我们第一次真正接触人工智能做好准备，也是因为许多不同的加入，从希望到恐惧。

人工智能革命不外乎是一个通过的仪式。但是要知道，这个旅程带我们去哪里，需要知道它从哪里开始。在大约十年的深度学习之后，是时候评估进展并回顾一些最重要的里程碑和剩余的挑战了。

[![AI Revolution](../Images/836bf918a4a71577004cd0b84493a609.png)T2】](https://www.frootlab.org/images/posts/AI-Revolution.png)

### [](#bigbang)大爆炸！

深度学习的出现可以追溯到 Geoffrey Hinton 的冒失鬼科学文章“[用神经网络](https://www.cs.toronto.edu/~hinton/science.pdf)降低数据的维度”(Hinton et al. 2006)。它的内容可以概括为两个重要的观察:

1.  某些被称为受限玻尔兹曼机器(RBM)的无向图形模型，可以通过最大化其可能性来有效地训练 <sup id="fnref:1">[1](#fn:1)</sup> 来表示数据。

2.  这些 RBM 可以堆叠在一起，以“预训练”深度人工神经网络(ANN)，该网络在随后的“微调”步骤中通常会获得比没有预训练时好得多的解决方案。

这些要点并不像它们看起来那样毫无生气:本质上，它们意味着几乎任何事情 <sup id="fnref:2">[2](#fn:2)</sup> 都可以用适当的数据来预测！数据科学家通常没有精力充沛的名声，但这一发现一定让他们喜极而泣！

### [](#the-bayesian-dbm)贝氏:DBM

虽然 Hinton 的文章明确地为深度人工神经网络铺平了道路，但它并没有对预训练的用途做出解释，也没有提供描述它的数学框架。尽管有这些缺点，Guillaume Desjardins 的团队还是大大改进了 Hinton 的方法，将一堆 RBM 焊接到一个单独的深层 Boltzmann 机器中(DBM)。

他们的文章“[关于训练深度玻尔兹曼机器](https://arxiv.org/abs/1203.4416)”(德斯贾丁斯，库维尔，本吉奥 2012)为堆叠的 RBM 的同时有效的 <sup id="fnref:3">[3](#fn:3)</sup> 训练提供了基于梯度的更新规则，因此避免了由它们的堆叠引起的损失。因此，堆叠的 RBM(DBM)被训练以通过保留其依赖性结构来生成训练数据的潜在表示。这种策略赋予了它高度的可推广性。

然而，除了这些改进之外，DBMs 还提供了关于预训练本质的重要提示:DBMs 通过最大化似然性来生成训练数据的样本分布。这可以被想象成一个流形的膨胀，它依赖于总体最小二乘回归的数据。然而，如果没有预先训练，人工神经网络只能执行普通的最小二乘回归，这严重削弱了它们的泛化能力。

### [](#the-frequentist-gan)惯犯:甘

> "对抗训练是自切片面包以来最酷的事情."——扬·勒昆

Hinton 文章中的另一个隐晦之处是一个无方向的图形模型的继承，接着是一个有方向的——这确实是最危险的部分！从表面上看，这两个模型的参数空间似乎是相当的，但它们根本不是！特别是对于不同的概率分布，它们产生。但是如何解决这个问题呢？想象一下两个孩子共享玩具:难怪他们总是吵架！一个关于 Ian Goodfellow 的小组提供了一个相当直接的解决方案:每个模型都有自己的参数空间！

文章“[生成对抗网络](https://arxiv.org/pdf/1406.2661.pdf)”(good fellow 等人，2014 年)提出了一个模型，其中一个人工神经网络被训练来生成样本分布，而另一个被训练来区分人工生成的样本和真实观察到的数据。因此，生成网络试图通过增加错误分类的比例来欺骗判别网络，而判别网络试图减少错误分类的比例，这是一个零和游戏。

由于这种方法，GANs 顺便解决了 DBMs 的另一个问题:由于 DBMs 的似然梯度通常不容易处理，所以必须通过马尔可夫链或变分推理来估计。然而，gan 不要求这样的估计。结果令人印象深刻！特别是，照片般逼真的图像和视频因人工[人脸](https://thispersondoesnotexist.com/)和[深度假像](https://en.wikipedia.org/wiki/Deepfake)而备受关注。

### [](#what-will-come-next)接下来会发生什么？

> “现在属于一起的将一起成长”——维利·勃兰特

深度模型的动物园呈指数增长！目前，我们发现自己被许多有前途的方法所包围，但上述两种方法---- DBMs 和 GANs ----之所以至关重要是有原因的:它们具有统计学中长期不和的学派的基本和纯粹的特征:Bayesians 派和 Frequentists 派。

在这一点上，人们可以将罗密欧与朱丽叶相提并论，这引发了将他们放在一起看看会发生什么的想法。瞧，有些人已经这样做了！这个方向的第一步，例如“[玻尔兹曼编码对抗机器](http://physics.bu.edu/~pankajm/PY895/BEAM.pdf)”(Fisher 等人 2018 年)令人印象深刻地证明了这种融合有很大的潜力！这不是偶然的，因为这两种方法在结构和表现上都显示出独特的优势。所以我敢打赌:深度学习的下一件大事是 GANs 和 DBMs 的融合。

但是让我们把这个预测进一步延伸到未来。有一件事迄今为止在深度学习中只受到了很少的关注:像 DBMs 这样的无向图形模型有能力捕捉依赖结构，不仅是枯燥的线性结构，而且确实是任何足够平滑的结构！然而，这一财产根本没有被开发！为什么？简单地说，在文献中有一个很大的空白，因为它影响统计以及微分几何和拓扑！尽管如此，我确信深层结构推理的可能性满足于努力发展一个全新的统计学分支 <sup id="fnref:4">[4](#fn:4)</sup> 。

通常我试着不去多愁善感，但是关于人工智能革命的前景不知何故可能是压倒性的。不管上述方面会变得多么重要，毕竟它们仍然只是等待我们的难以置信的进步中的一个小小的篇章。

### [](#footnotes)脚注

<sup id="fn:1">1。由于 RBMs 的二分图结构，重复的 Gibbs 抽样快速混合，这允许对数似然梯度的有效近似。[↩](#fnref:1 "Jump back to footnote 1 in the text.")T3】</sup>

<sup id="fn:2">2。要求可观测量描绘出足够平滑和李普希兹连续的轨迹[↩](#fnref:2 "Jump back to footnote 2 in the text.")T3】</sup>

<sup id="fn:3">3。如果你把二分图堆在一起，你还是会得到一个二分图。当然，这有点复杂，但在引擎盖下，这就是为什么 DBMs 可以有效地训练的原因。[↩](#fnref:3 "Jump back to footnote 3 in the text.")T3】</sup>

<sup id="fn:4">4。我开始了将统计学与微分几何和拓扑学相结合的旅程，如果我可以用我的想法来启发你，我会很高兴:[ [pdf](https://drive.google.com/open?id=1RnRLM7WlSw63zuftRassTI18ohMjr0vE) ， [pdf](https://drive.google.com/open?id=1nkNFPLXrAigD3MsETqt5hN9VI94nLvN0) ， [pdf](https://drive.google.com/open?id=16gl2GCT5taeH9oo86SHkFKZdeTyRRwTs) ， [pdf](https://drive.google.com/open?id=1jssUKKcUFw4LfDiWqjneMKRvVFUmZffP) ]，但请注意:你可能会发疯(或感到无聊)！[↩](#fnref:4 "Jump back to footnote 4 in the text.")t11】</sup>