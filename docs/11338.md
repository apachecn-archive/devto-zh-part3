# 缓存:吉拉云中的性能工程

> 原文:[https://dev . to/atlassian/caching-in-performance-engineering-in-jira-cloud-22ch](https://dev.to/atlassian/caching-in-performance-engineering-in-jira-cloud-22ch)

当您为来自全球各个角落的数百万用户提供服务时，性能工程是一件大事。我们之前写了一篇关于吉拉& Confluence 的大型工程改造项目的文章，我们将其命名为 Vertigo——点击阅读更多关于整个项目的内容。

作为眩晕计划的一部分，我们知道我们将不得不在表演上投入大量的工程努力，尤其是吉拉的表演。虽然这些年来我们一直在利用现有的工具和架构来提高吉拉的性能，但 Vertigo 架构带来了大量新的机会来进一步提高吉拉的性能和可靠性。

现在是时候分享我们在 Vertigo 计划期间所做的一些工作，以提高吉拉在新平台上的性能。这不仅仅是我们取得的收益，而是我们如何重新思考我们的一些旧策略(如在应用程序内存中缓存数据)，以便在需要不同方法的新底层架构上取得成功。

通过在 AWS 上的云中运行吉拉而释放的新功能使我们获得了巨大的性能和可靠性收益。虽然我们对迄今为止所取得的进步感到自豪，但我们知道要提高吉拉云的性能还有很多工作要做。我们将表演视为一个持续的旅程，并将继续在吉拉表演的各个方面投入大量精力。

## [](#betting-big-on-zero-affinity)在零亲和度上下大赌注

眩晕架构的目标之一是实现“零亲和力”。这意味着任何运行在负载均衡器后面的吉拉应用程序节点都可以处理传入的请求。这种架构有许多好处，零关联性是广泛使用的 [12 因素应用模型](https://12factor.net/)中包含的概念之一。

从性能角度来看，零关联性的一个主要优势是能够横向扩展处理请求的应用程序节点数量，以响应增加的负载。这意味着我们能够更好地在我们的基础设施上处理更大的吉拉客户，并且是我们能够将吉拉云的最大许可证层从 2，000 个用户增加到 5，000 个用户的关键促成因素之一。

在吉拉服务器和我们之前的吉拉云架构(代号为 Unicorn)中，频繁使用的客户数据跨请求缓存在内存中，这提供了显著的性能优势，因为缓存的数据可以在几微秒内访问。然而，我们对内存缓存的严重依赖也带来了一些问题，特别是当数据变得太大而无法保存在内存缓存中时，这有时会导致性能下降。

为了让我们迁移到零关联性架构，我们现在必须从吉拉应用程序节点中移除所有这些长期内存缓存。这带来了性能挑战，因为以前可以在几微秒内查找的数据现在必须从远程数据库获取。为了解决我们的性能挑战，我们采用了许多模式:

*   对于在请求过程中需要重用的数据，将其保存在内存中请求范围的缓存中
*   对于跨请求可重用且计算成本高的数据，我们会将其存储在 Memcached 中
*   对于存在于数据库中的数据，我们将专注于改变我们的查询，以更有效地利用我们的 Postgres 数据库

## [](#challenges-in-data-access-patterns)数据访问模式的挑战

我们首先移除所有内存中的跨请求缓存，开始了我们的零关联性之旅。跨请求缓存被替换为保存在每个吉拉应用程序节点的内存中的请求范围的缓存。这允许我们对请求中经常使用的信息进行一次数据库调用，例如关于当前用户的元数据。一旦这项工作完成，我们就能够调查并了解我们的性能问题在哪里。我们期望看到的是性能下降，因为跨请求缓存的丢失意味着我们必须更频繁地与数据库对话，而不是从内存存储中读取数据。

我们看到了一种常见的挑战模式，我们称之为“n+1”访问模式，在这种模式下，我们首先向数据库发出请求以获取一组对象，然后进行后续调用以逐个获取其他数据。例如，以下面的代码片段为例:

```
// Gets a list of Projects that are 'special'
Collection projects = projectManager.getAllProjects()
return projects.stream()

// Each call to #isSpecial calls the database
.filter(p -> specialProjectManager.isSpecial(p)).collect() 
```

上面的代码演示了一个在吉拉代码库中广泛使用的典型模式。以前这不是一个性能问题，因为`ProjectManager`和`SpecialProjectManager`都实现了一个缓存层，允许它们基于内存中的数据返回结果。当这些缓存从跨请求变为请求范围时，我们会看到许多缓存不再有效，因为每个请求只命中一次。

回到上面的代码示例，我们必须知道更多的信息来确定性能开销:

*   调用数据库的开销是多少？
*   项目列表有多大？

在我们的架构下，我们看到对数据库的调用通常需要 0.5-2 毫秒的网络时间。我们有高网络时间的原因是我们在多个可用性区域中部署了吉拉应用程序节点，并且我们在 RDS 数据库上启用了高可用性功能，这意味着我们必须经常进行跨 AZ 请求才能联系数据库。

除了网络时间，还有数据库上的查询执行时间。对于简单的查询，比如按 ID 查找，我们发现网络开销通常大于查询时间，并且从吉拉应用程序端来看，我们通常预期平均总查询时间为 1 毫秒。

至于项目列表的大小，这在不同的客户规模之间有很大的不同:

[![Before performance-tuning, latency for large Jira customers was significant.](../Images/14681676c770ba237417f82760cd65bb.png)T2】](https://atlassianblog.wpengine.com/wp-content/uploads/2019/02/perfbefore.png)

这显示了“n+1”数据访问模式的性能开销是如何随着客户规模的不同而显著变化的。对于小客户，开销是存在的，但不显著，而对于大客户，开销变得很大。

### [](#bulk-loading-of-data)批量加载数据

我们用来修复“n+1”数据访问模式的相当一致的模式是通过创建只需要一次(或某个常量)数据库调用的新方法来批量加载数据。以上面加载特殊项目的例子为例，现在看起来可能是这样的:

```
// Gets a list of Projects that are 'special'
Collection<Project> projects = projectManager.getAllProjects()

return specialProjectManager.filterSpecialProjects(projects) 
```

新查询的执行时间可能会比以前稍慢，可能会多花几毫秒。然而，新方法的整体性能提升将是巨大的，尤其是对于拥有较大数据集的客户。

**[![](../Images/a9c8e03da8f3e5c891709f1af2202219.png)](https://atlassianblog.wpengine.com/wp-content/uploads/2019/02/perfafter.png)T4】**

这种批量装载的模式非常有效。然而，我们发现在实现这些性能改进的过程中存在许多挑战:

*   找到“n+1”中涉及的代码路径通常很困难，因为调用路径可能分布在许多类中
*   在某些情况下，从数据库中提取的数据量过大(例如，数万行)，我们必须使用另一种方法
*   “n+1”性能问题的表面积非常大，这意味着我们必须找到一种明确的方法来区分工作的优先级。

## [](#developing-observability-tools)开发观察工具

为了开始在我们的新平台上发现性能问题，我们从几个内部实例中克隆了数据，并将它们导入到 Vertigo 平台上。这为我们提供了一个很好的数据集来查找导致不良性能的代码路径。

一旦我们确定可以使用这些数据集重现缓慢的请求，我们就必须确定如何获取缓慢的请求，并将其映射到代码中的根本原因。我们从利用数据库连接池中的工具开始， [Vibur DBCP](http://www.vibur.org/) ，它允许我们跟踪在请求期间执行的所有查询。这些查询被标准化并分组在一起，并与连接获取时间、查询执行时间和结果集处理时间的度量相关联。然后，这些数据通过我们的日志记录管道发送到 Splunk，在 Splunk 中，我们构建了仪表板，以提供跨请求聚合视图和单个请求的深入查看。

这些仪表板非常有帮助，让我们可以看到哪些请求由于 n+1 问题而变慢了。然而，我们面临的挑战是将查询与导致这个性能问题的代码关联起来。虽然很容易找到执行实际查询的存储层，但是方法调用中的代码循环可能需要删除许多层。

为了帮助我们识别导致每个请求大量 DB 调用的确切代码，我们开发了一个工具，我们称之为 DB Stack Trie。这个想法是，我们将在每次访问数据库时捕获堆栈跟踪，并将堆栈跟踪聚集到一个加权的 [Trie](https://en.wikipedia.org/wiki/Trie) 数据结构中，其中节点代表文件名、行号和 DB 调用数，边代表调用堆栈分支的位置。为了帮助我们可视化单个请求中 DB 调用的分解，我们创建了一个[树形图](https://en.wikipedia.org/wiki/Treemapping)。

下面显示了一个生成的树形图示例，该图在研究如何加速 CSV 问题导出的过程中使用。这是一个特别极端的例子，因为这个请求需要 9 万多次数据库调用！

[![](../Images/8e5f22b135020f7f9567027665088839.png)T2】](https://atlassianblog.wpengine.com/wp-content/uploads/2019/03/image.png)

总之，这个工具使我们能够识别高影响性能的问题，并确定根本原因。然后，我们重构了代码，以利用诸如批量加载数据之类的模式，或者将计算推入数据库的更有效的查询。

## [](#improving-application-node-performance)提升应用节点性能

随着我们不断地改进数据库访问模式的性能，我们看到数据库所用的时间比例在减少。这是一个很大的进步，我们对看到的性能改进非常满意，但是请求时间仍然比我们预期的要长。为了解决这个问题，我们开始调查吉拉应用程序内部或运行应用程序节点的硬件上存在的潜在性能瓶颈。

### [](#splitting-types-of-workloads)拆分工作负载类型

最初，我们的吉拉应用程序被部署为 EC2 上的同构机群，自动伸缩组中的每个节点都有相同的配置。这是一个次优配置，因为我们在吉拉有多种类型的工作负载:

*   我们希望以低延迟响应的入站 web 服务器请求(例如页面加载、REST 请求)
*   后台任务(例如，发送电子邮件和 webhooks，或与外部系统执行同步)具有高吞吐量要求，但延迟要求不太严格

因为相同的吉拉应用程序节点负责完成两种工作负载，所以我们无法针对任一特定使用情形调整我们的配置。为了解决这个问题，我们将我们的部署分成两个可以独立自动扩展的组:

*   针对低延迟响应进行了优化的 web 服务器节点
*   针对高吞吐量进行了优化的工作节点

webserver 和 Worker 节点将运行相同的代码(但具有不同的配置参数),并在发布更改时并行部署。

这次拆分的结果非常积极。通过将后台负载从所有吉拉节点转移到工作节点，然后针对吞吐量优化这些节点，我们能够将服务峰值负载所需的节点总数减少约 40%。

[![](../Images/afdaa8865e1b8eab54e11eb18fb749e8.png)T2】](https://atlassianblog.wpengine.com/wp-content/uploads/2019/02/jirashardadg3.png)

## [](#cpu-profiling)CPU 剖析

吉拉网络服务器节点部署在 c 4.4x 大型 EC2 实例上，并提高了 CPU 利用率。这个扩展规则是基于实验发现的，我们发现一旦平均 CPU 使用率超过这个阈值，性能就会明显下降。这一观察表明:

*   我们的性能很可能受限于 CPU，或者受限于与 CPU 相关的另一个资源
*   如果我们的应用程序确实受到 CPU 的限制，那么就有可能通过修复 CPU 瓶颈来改善请求延迟

我们开始调查是否有任何非 CPU 资源正在产生性能瓶颈，检查诸如网络 IO 和磁盘 IO 之类的东西。我们的调查表明，在这些资源上没有瓶颈，因此我们需要更深入地调查我们的 CPU 使用情况。

### [](#flame-graphs)火焰图

为了调查我们的 CPU 使用情况并详细了解生产中的行为，我们决定利用 Java 火焰图，正如网飞所描述的[。这些提供了许多好处:](https://medium.com/netflix-techblog/java-in-flames-e763b3d32166)

*   这种类型的分析增加了较低的开销，这意味着我们可以在生产中使用它们
*   CPU 配置文件涵盖了系统上执行的所有代码，这意味着我们可以看到非 Java 代码是否正在消耗 CPU

我们选择了几个生产吉拉 web 服务器节点，并在这些节点上运行火焰图配置文件，我们得到的结果类似于下图:

[![](../Images/c738a2f8f2abdb6b897f422a511bc414.png)T2】](https://atlassianblog.wpengine.com/wp-content/uploads/2019/03/image-1.png)

我们立即注意到,`__clock_gettime`占据了整个 CPU 的出乎意料的大部分。此外，我们注意到 get time 调用正在进行系统调用。为了解决这个问题，我们研究了将时钟源改为硬件支持的时钟源，在本例中为`tsc`(时间戳计数器)。关于时钟源的更多信息，请查看这篇来自 redhat 的[文章。](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/7/html/reference_guide/chap-timestamping)

我们首先通过运行:
来测试单个自动缩放组中节点上的时钟源变化

```
echo 'tsc' > 
/sys/devices/system/clocksource/clocksource0/current\_clocksource 
```

在此之后，我们能够重新运行火焰图来比较结果:

[![](../Images/599deb9311a8def1802c7e071d820566.png)T2】](https://atlassianblog.wpengine.com/wp-content/uploads/2019/03/image-2.png)

当比较这两个火焰图时，CPU 使用率与`__cloud_gettime`的差异非常明显。我们还可以注意到，在两个火焰图之间，Java 代码的 CPU 使用率似乎有所增加。发生这种情况是因为当我们启用`tsc`时钟源时，CPU 使用率发生了巨大的变化，足以让我们的自动缩放规则发挥作用，并减少自动缩放组的大小。

事实证明，其他人也发现了 Xen clocksource 的类似性能问题，例如来自网飞的 Brendan Gregg 在 2014 年谈到了这个问题。事实上，这是 AWS 对现代实例类的默认[推荐](https://www.slideshare.net/AmazonWebServices/cmp402-amazon-ec2-instances-deep-dive/24)。

### [](#overly-interrupted-cpu-cores)过度中断的 CPU 核心

作为我们对 CPU 行为调查的一部分，我们注意到一些 CPU 内核的使用率高于平均水平。虽然如果这是随机和短暂的，这并不令人惊讶，但我们观察到的是，在所有吉拉应用节点中，内核 5 和 6 一直报告较高的利用率。这种一致性表明，有一个潜在的根本原因导致这些 CPU 内核被更频繁地使用。

凭直觉，我们怀疑网络中断行为可能导致特定内核的高利用率。虽然我们已经做了大量的工作来减少每个请求的数据库调用次数，但是吉拉应用程序仍然与数据库通信频繁，这将导致每个请求的大量网络中断。为了获得更多信息，我们检查了

```
/proc/interrupts 
```

我们看到的是两个 CPU 内核处理所有的中断——这是 CPUs 5 和 6！

因此，我们有一个可能导致 CPU 核心利用率不平等的潜在原因，但现在我们必须找出如何更均匀地分配负载。幸运的是，Linux 有一个名为[接收包导向](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/performance_tuning_guide/network-rps) (RPS)的特性，它允许我们定义不同的 CPU 内核将如何处理传入的网络包。默认情况下，这是禁用的，接收网络中断的内核也会处理数据包。为了减少 CPUs 5 和 6 上的负载，我们尝试设置 RPS 配置，将传入的数据包处理导向所有其他 CPU。这种方法正如我们所希望的那样，成功地以一种更加平均的方式分配了我们的 CPU 利用率。

### [](#ec2-c5-instances)EC2 C5 实例

在调查潜在 CPU 瓶颈的同时，我们的团队还在调查为 EC2 使用新的 C5 实例类，这是在 AWS re:Invent 2017 上发布的。新的实例类旨在提供改进的 CPU 性能，并包括一个名为 [Nitro](https://www.youtube.com/watch?v=LabltEXk0VQ) 的全新管理程序。

Nitro hypervisor 基于 Linux KVM，并提供了一个我们特别感兴趣的特性——KVM clock(T2)。在此阶段，我们仍在研究在生产中一致启用`tsc`时钟源和接收包控制配置的方法，以解决上述 CPU 瓶颈。

我们首先在我们的登台环境中测试了 C5 . 4x 大型实例，然后将它们部署到我们的一个生产环境中，以获得一些性能数据。我们立即看到，在许多不同的请求类型中，服务器响应时间的性能有了显著的提高。下图显示了为吉拉观点问题页面提供服务的服务器端请求延迟的性能变化，其中我们看到在第 50、75 和 90 个百分点上下降了大约 30%。

[![A graph showing performance engineering data.](../Images/5b3cca0211a621d572721b9b33543092.png)T2】](https://atlassianblog.wpengine.com/wp-content/uploads/2019/03/image-20190110-022414.png)

我们的监控还显示，在 C5 实例类上，CPU 负载现在平均分布在可用内核上。为了验证由少量 CPU 处理中断的问题已经解决，我们再次运行了`/proc/interrupts`。这表明 CPUs 3 到 10 处理网络中断的分布大致均匀。

## [](#the-journey-continues)旅程继续！

新的 Vertigo 架构带来了许多性能挑战，但也为我们提供了更多提高吉拉性能和可靠性的机会。通过投资可观察性工具，我们能够了解我们系统的性能，并做出必要的改变以获得巨大的性能改进。

虽然我们在提高吉拉云的性能方面取得了长足的进步，但我们知道，要继续提高性能，我们还有很多工作要做。这篇博客关注了我们在吉拉服务器上为提高性能所做的工作，但是也有大量的努力来提高我们前端的性能。

在未来的博客中，我们将进一步讨论其他性能改进流，如提高页面加载的前端性能，以及如何实现性能改进，使吉拉能够更好地为我们最大的客户进行扩展。

。。。

附:如果你喜欢埋头研究这种有趣的工程问题，我们正在招聘。很多。只是说说而已。

帖子[缓存在:吉拉云中的性能工程](https://www.atlassian.com/blog/technology/jira-performance-engineering)最早出现在由 Atlassian 撰写的[工作生活上。](https://www.atlassian.com/blog)