# 通过批量删除清除数据

> 原文:[https://dev.to/am2/purging-data-with-batched-deletes-16jp](https://dev.to/am2/purging-data-with-batched-deletes-16jp)

<figure>[![](../Images/4109b260b742f6491aa449cb0609a809.png)](https://www.youtube.com/watch?v=4VO1nYGHw0c) 

<figcaption id="caption-attachment-1739">这个人知道批量删除是有帮助的。</figcaption>

</figure>

昨天，我写了关于[计划数据清除的重要性](https://dev.to/2019/04/the-thrill-of-purging-old-data/)。今天，让我们来看一下清除数据的最简单要求:在数据创建后将其保留 X 天。在很多情况下，这是 T2 的规则。日志数据、遥测数据，甚至是事务性数据，通常从创建之日起保留 X 天。

鉴于这种情况非常普遍，让我们更详细地讨论一下我想删除它。删除数据很简单吧？

```
DELETE t 
FROM dbo.Transactions AS t 
WHERE CreateDate <= DATEADD(dd,-90,GETDATE()); 
```

## [](#not-so-fast)没那么快……

“一个大的删除”语句很容易编写，但是对于我们大多数人来说，如果删除多行，可能会引起一些问题。我说的是删除，但这同样适用于所有 DML——插入、更新和删除。

另一种方法是批量删除(或者批量插入或者批量更新)。批处理需要更多的努力，而且你和我一样懒，你会想证明这种额外的努力是值得的…所以，我想了想，下面是迫使我做批处理的五个原因

### [](#replication)复制

每个人都讨厌复制，但我是它的忠实粉丝。复制的一个恼人之处是，发布服务器上的大删除变成了订阅服务器上的 [RBAR](https://sqlstudies.com/2016/08/17/rbar-vs-batch/) 删除。比如说，对 100 万行进行一次大的删除，将会用 100 万条单独的`DELETE`语句阻塞您的复制分布，阻塞您的复制分布，减慢需要复制的所有其他内容。

如果批量删除，订阅服务器上仍然会有 100 万个 RBAR 删除，但是其他内容将有机会与其他命令穿插在一起，并将保持数据向订阅服务器移动——分散延迟，而不是让一个大的删除造成一个大的死锁

### [](#availability-groups)可用性组

在可用性组上，事务以同步或异步模式提交到 AG 辅助节点，具体取决于您的配置。该提交将事务强化到辅助节点的事务日志中，而不是数据文件中。这些事务从日志到数据文件的重放**总是一个异步过程**。

长时间运行的事务可能会导致辅助重做落后。如果辅助重做落后，故障切换将需要更长时间。假设一个大的删除需要 20 分钟。在第 19 分钟，AG 出现故障转移。在辅助节点可用之前，辅助节点将有更多的事务日志需要在故障转移时进行协调。即使它在您的大删除提交后的第*天进行故障转移，辅助节点也会忙于赶上。结果是您的故障转移将花费更长的时间。更长的故障转移意味着故障转移期间更长的停机时间。如果您的故障转移花费的时间*太长*，您就有在故障转移期间违反正常运行时间 SLA 的风险。*

### [](#transaction-log-size)事务日志大小

长时间运行的事务会导致事务日志增长。SQL Server 只能清除比最早的活动事务旧的日志。那是什么意思？如果您最长的运行事务需要 60 分钟，那么您就不能清除这整整 60 分钟的事务日志——即使您备份了事务日志，或者处于[简单恢复](https://docs.microsoft.com/en-us/sql/relational-databases/backup-restore/recovery-models-sql-server)中。

通过将大型操作批处理成较小的块，可以更好地管理事务日志，防止它变得太大，从而允许重用现有的日志。

### [](#premature-finish-starting-over-vs-finishing-later)过早结束:重新开始 vs 后期结束

如果您的清除过程没有完成会发生什么。也许它命中了一个错误，或者出于某种原因你必须杀死它。如果你正在做一个大的删除，那么整个事情都会回滚。这意味着明天或下周，你必须从头再做一遍，加上在此期间过时的新数据。

如果您是批量删除，只有当前批次必须回滚；当你准备好重新开始时，你从你停下的地方继续。

### [](#blocking)闭锁

长事务更容易导致阻塞。您进入、修改数据和退出的速度越快，导致与阻塞相关的问题的可能性就越小。根据您的隔离级别，实际影响会有所不同。

即使您从聚集索引的一端清除，而活动数据在另一端，您仍然可以看到长时间运行的删除的阻塞。即使在清理聚集索引的非活动部分时，也可能会在整个索引中修改一些非聚集索引。

在每个批处理操作之间，您让其他会话进来做它们的工作。理想情况下，所有那些被阻塞的会话都可以在批处理之间跳转并完成它们的工作。有时，您可能需要人为地限制清除，以增加删除之间的时间间隔。目标是确保任何阻塞持续时间短，并完全解决，而不是随着时间的推移继续堆积。

## [](#batching-to-the-rescue)分批前往救援

批处理成块有助于缓解长时间运行的事务导致的所有这些问题。确定你的批量是一个平衡的行为。执行一次大的删除将是最快、最有效的删除方式，除了上面列出的可能的缺点。执行单行删除将是最慢、效率最低的删除方式，除非它解决了所有这些缺点。解决方案是找到足够大的批量以提高效率，但又足够小以尽量减少问题。根据您的环境、数据和 SLA，可能是 5，000 行一批，也可能是 5，000，000 行一批。

在每个批次之间，您甚至可能想要暂停并等待几毫秒。在批次之间快速呼吸有助于进一步缓解问题。在这短暂的等待期间，被阻塞的事务可以继续进行，复制可以跟上，并且它会降低数据更改的速度，以防止您过快地填满事务日志。你要等多久？我不能告诉你——这和设置批量大小是同一个平衡过程的一部分。你需要平衡你需要进行的速度和你需要停下来让其他进程进行的程度。这完全取决于你在你的环境中做什么。

明天，我们将看一个我喜欢用来做批量删除的模式。

* * *

#### [](#additional-reading)补充阅读

*   [清除旧数据的快感](https://dev.to/2019/04/the-thrill-of-purging-old-data/)
*   [根据创建日期清除数据的代码](https://dev.to/2019/04/code-to-purge-data-on-creation-date/)
*   [DBA 数据库 Github Repo](https://github.com/amtwo/dba-database)

帖子[用批量删除](https://am2.co/2019/04/purging-data-with-batched-deletes/)清除数据最早出现在[安迪·M·马龙-阿姆](https://am2.co)上。