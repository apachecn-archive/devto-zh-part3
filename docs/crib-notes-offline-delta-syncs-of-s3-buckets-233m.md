# S3 桶的离线增量同步

> 原文：<https://dev.to/ferricoxide/crib-notes-offline-delta-syncs-of-s3-buckets-233m>

在正常情况下，同步两个桶就像做`aws s3 sync <SYNC_OPTIONS> <SOURCE_BUCKET> <DESTINATION_BUCKET>`一样简单。然而，由于我的一些客户的信息安全需求，有时需要在两个 S3 存储桶之间执行数据同步，但是使用相当于“离线”传输的方法。

为了说明“离线”的含义:

1.  从数据源创建传输档案
2.  跨越安全边界复制传输档案
3.  将转移存档文件拆包到其最终目的地

请注意，事情比过程的总结要复杂一些——但是这给了你主要努力点的要点。

第一次进行离线存储桶同步时，目标通常是传输整个存储桶。然而，对于刷新同步——特别是对于大于普通内容大小的存储桶，这可能不太理想。例如，可能有必要对每月增长几千兆字节的存储桶进行每月同步。一年后，完全同步可能意味着必须移动几十到几百千兆字节。更好的方法是只同步增量——只复制当前和之前同步任务之间的变化(几 GiB 而不是几十到几百)。

AWS CLI 工具实际上并没有“仅同步自`<DATE>`以来添加/修改的文件”。也就是说，解决这个问题并不困难。像下面这样的简单 shell 脚本很有用:

```
for FILE in $( aws s3 ls --recursive s3://<SOURCE_BUCKET>/ | \
   awk '$1 > "2019-03-01 00:00:00" {print $4}' )
do
   echo "Downloading ${FILE}"
   install -bDm 000644 <( aws s3 cp "s3://<SOURCE_BUCKET>/${FILE}" - ) \
     "<STAGING_DIR>/${FILE}"
done 
```

为了解释以上内容:

1.  创建要迭代的文件列表:
    1.  使用`$()`符号调用子流程。在该子流程中...
    2.  调用 AWS CLI 的 S3 模块以递归方式列出源桶的内容(`aws s3 ls --recursive`)
    3.  将输出传送到`awk`——查找比`s3 ls`的第一个输出列(文件修改日期列)中的值更新的日期字符串，并只打印第四列(S3 对象路径)。子流程的输出被捕获为一个可迭代的列表结构
2.  使用一个`for`循环方法迭代先前组装的列表，将每个 S3 对象路径分配给`${FILE}`变量
3.  因为我讨厌让程序安静地做事情(我不相信它们不会挂起)，所以我的第一个循环命令是通过`echo "Downloading ${FILE}"`指令说出发生了什么。
4.  `install`行利用了 BASH 和 AWS CLI 的 S3 命令中的一些优点:

    1.  通过指定“`-`”作为文件复制操作的“目的地”，您告诉 S3 命令将获取的对象内容写入到`STDOUT`。
    2.  BASH 允许您获取一个输出流，并通过用`<( )`包围产生输出的命令来为它分配一个文件句柄。
    3.  调用带有`-D`标志的`install`命令告诉该命令“创建所有必要的路径元素，将源文件‘file’放在文件系统中的所需位置，即使中间的目录结构还不存在。”

    将所有这些放在一起，`install`操作获取流式的`s3 cp`输出，并将其作为一个文件(模式 000644)安装在从`STAGING_DIR`加上 S3 对象路径派生的位置...从而在`STAGING_DIR`中保留了`SOURCE_BUCKET`的内容结构

显然，这种方法只适用于附加/替代增量。如果您需要考虑删除和/或移动，这种方法是不够的。