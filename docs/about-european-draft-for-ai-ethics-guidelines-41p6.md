# 关于人工智能伦理指南的欧洲草案

> 原文：<https://dev.to/shamar/about-european-draft-for-ai-ethics-guidelines-41p6>

以下是我发给致力于为欧洲委员会定义人工智能伦理准则的高级专家小组的评论。

要理解它们，你需要对人工智能和机器学习有一个相当好的理解，并阅读[可信人工智能伦理指南草案](https://ec.europa.eu/futurium/en/system/files/ged/ai_hleg_draft_ethics_guidelines_18_december.pdf)

# 引言:指导方针的基本原理和前瞻性

导言的前几行强调了草案的一个严重缺陷:支撑委员会愿景的支柱显示出一种根本性的偏见:

1.  增加对人工智能的公共和私人投资，以促进其应用，
2.  为社会经济变革做准备，以及
3.  确保一个适当的道德和法律框架，以加强欧洲价值观

急于采用一项被广泛误解的技术显然抑制了对其局限性和风险进行推理的能力。

在试图推动它的采用之前，委员会应该试图理解人工智能保护伞下的一套技术应该在多大程度上和在哪个领域进行实验。

正如 Shoshana Zuboff 最近所写的，技术不是不可阻挡的自然力量，而是为特定人类的利益和需求服务的人工制品。换句话说，技术是通过其他方式对政治的起诉:每一项进步都可以被设计成服务于公共利益或私人利益和精英利益。就像政治一样，放弃参与其进程就意味着受制于他人的意志。

在谈论“可信的人工智能”之前，我们应该有一个能够理解这个话题的人群，他们的信任是有意义的。

就今天而言，如果不对学校进行认真的投资，以培养历史和信息学作为我们公民身份的先决条件，这种信任就不会有意义，而只会是欺骗性的和毫无根据的。

这不是对技术的信任，而是对公司和“专家”的信任，他们可以利用这种信任和对这个问题的普遍无知来削弱监管，加强对社会的控制。

话虽如此，对伦理框架的高层次描述基本上是合理的:有理由认为，当所有人都能够理解神经网络的校准与 k 均值聚类的不同时，类似的框架就会出现。

然而，介绍之前的术语表已经表明，我们还没有为这样的框架做好准备:尽管这些定义是由一个高级人工智能专家组编写的，但它们仍然使用一种抽象语言来描述软件。特别是将软件缺陷(有意或无意的)描述为“偏见”,表明了对所讨论的软件以及定义其行为的统计过程的深刻误解。后来，当草案在谈论由确定性机器(又名计算机)执行的软件时引用“非确定性”时，类似的担忧出现了。

这种语言令人担忧，因为它显示了 HLEG 将风险合理化为不可避免的趋势，而不是深入理解并考虑这些风险。

# 第一章:尊重基本权利、原则和价值观——道德目的

尽管有一个有趣和方便的介绍，这一章提出的原则缺乏一个基本的层次结构。

通过查看这些原则，应该很明显:

*   慈善的原则:“行善”
*   无害原则:“不造成伤害”
*   自主原则:“保留人的能动性”
*   正义原则:“公平”
*   可解释原则:“透明运作”

即使我们没有两千多年的希波克拉底誓言和几代医生伴随着“不伤害”的格言成长，我们也可以看到最后三个原则只是更普遍的“不伤害”的专门化。特别是，自主原则试图解决个人面临的风险，公正原则试图解决弱势群体面临的风险，可解释原则试图解决社会政治风险。

既然无罪的原则如此重要，需要三个专业化，我们应该把它放在第一位，在有益的原则之前，并强调它与其他原则的关系:

*   无害原则:“不造成伤害”
    *   自主原则:“保留人的能动性”
    *   正义原则:“公平”
    *   可解释原则:“透明运作”
*   慈善的原则:“行善”

通往地狱的道路是由良好的意愿铺成的:就像医学一样，只要存在更简单、更安全的解决方案，它们就应该比更复杂、更危险的方案更受青睐。

但这份清单中有一个更重要的遗漏:人类最终责任的原则。

这是所有欧洲伦理和法律体系的一个基本原则:至少一个人必须对人为造成的问题负责。

换句话说:禁止人类做的事情不能通过人工代理来实现，不管这样的代理有多“自治”(也就是调试起来很昂贵)。

如果我们不准备实施这一简单而基本的人类责任原则，谈论道德是没有意义的。

关于“致命自主武器系统”的一节与上述所有原则形成鲜明对比。

一个伦理框架在提出诸如“不伤害”、“保护人类机构”、“公平”、“透明运作”和“行善”等原则时，唯一可信的方式是明确声明在欧洲领土上必须禁止自主武器系统(致命与否)。

“潜在的长期担忧”部分显示了通常基于科幻小说的担忧，这是当前炒作的另一面。

我们不应该担心人工意识，因为伪造人工意识要比实现人工意识容易得多。我们应该担心掌握着地球大部分财富的一小撮人手中的半自动武器。在计算这类武器时，我们显然应该包括每一种可以用来引导人类注意力、操纵感情或感知以及制造大众舆论的工具。

# 第二章:实现可信的人工智能

甚至这一章也提出了几个问题:

1.  关于“问责制”简短段落建议设计从金钱补偿到道歉的机制，但它忘记了包括监狱:为了获得信任，重要的是明确声明自主代理不能成为“免费出狱”的入场券。
2.  关于“安全”的部分看起来像是被设计成无效的:评估与使用基于人工智能的产品和服务相关的潜在风险而不定义当事情出错时的严重惩罚是毫无意义的。
3.  关于“透明性”的部分过于含糊和宽容:一个更简单的方法是说在消耗人类数据的应用程序中不允许不透明。这种规则将立即飙升人工智能研究的私人和公共投资，寻找可以完全解释和调试的新机器学习技术。
4.  关于“鲁棒性”的部分看起来设计得很好，但当它不恰当地谈论“非确定性”(错误，如果我们谈论的是确定性的、非量子的计算机)，并引用“复杂性”、“不透明性”和“对训练/模型构建条件的敏感性”作为不可重复结果的一种理由时，它会导致广泛的不负责任。简单地说，只要这样的条件存在，人工智能程序就不健壮，不应该应用于需要这样健壮的问题。
5.  关于“人类自主”的部分非常可怕:人绝不应该被机器推搡。如果人工智能像承诺的那样成功地增加了人类的财富，许多有大量空闲时间的友好的人将能够按我们的要求推动我们，但无论目标是什么，让有缺陷的机器操纵人类都太危险了:每个软件都有缺陷和漏洞，许多都有故意的后门:人工智能不会不同。

后来，在“可信人工智能的体系结构”中，在考虑确保道德行为的技术手段时，HLEG 建议在随机系统的“感觉”阶段整合一个道德信号。

这既幼稚又怪异:

*   我们应该使用哪种伦理？如果我们按照某种道德模式广泛部署自主机器，人们会适应它(因为机器无法真正适应我们):这可能会成为有史以来最有效的洗脑项目。人类自然适应周围的智能:在每个房间里放一个消费者代理，你将建立一个消费者群体。
*   多少伦理？谁来决定这个信号的权重？而当一个 bug 会抑制它的时候呢？或者如果其他输入淹没了这样的信号呢？在一个自治系统中，道德信号的唯一用途是保护企业免于承担错误的全部责任:假装向电车教授道德是愚蠢的，我们应该建立简单地防止致命事件发生的基础设施。

此外，在关于监管的部分，我们没有提到任何刑事司法:就像以前一样，应该清楚地说明，当一个自主神器杀人或伤害时，一个或多个人类将对此承担全部责任。

# 第三章:评估可信 AI

我真的很欣赏评估过程的灵活方法:谈到道德，一个清单太容易被利用了。

当然，每种技术都需要不同类型的评估:例如，用于校准 k 均值的数据集可能足以重现校准过程并排除任何种族歧视，但它完全不足以评估基于人工神经网络的分类器的任何属性。

然而，风险在于，在没有对人工智能技术的广泛理解的情况下，欧盟委员会将问狼们如何统治羊群:我们不能依赖咨询大公司的专家来将任何“信任”评估定义为可以操纵人们的东西。

此外，能够评估“值得信赖的人工智能”的道德标准，不能取代建立算法在获得人类数据之前必须具备的特征的明确监管。

特别是，我们需要将每个人工智能处理“有关逻辑的有意义信息”的权利扩展到受 GDPR 第 13 条保护的个人之外:即使是家庭、邻居、客户等群体也应该有权知道和理解应用于他们集体数据的确切逻辑，以及处理发生的时间和目的。

# 总评

尽管存在上述所有问题，我还是对专家组在起草这份草案时所付出的努力和关心表示赞赏。

对于欧洲来说，填补我们与美国和中国的技术差距是很重要的，看到认真的人致力于人工智能应用中出现的伦理问题也是令人欣慰的。

然而，更重要的是避免走捷径。善意和诚实是基础，但不足以平衡游说和炒作。

为了解决我们的技术问题(包括人工智能的采用)，我们需要提高大众对信息学的理解。我们需要一个新的大众教育计划，从小学开始对教师和教授进行大量投资。我们需要培养一代能够修改他们使用的软件的人，他们用自己的数据来养活自己。

既然技术就是政治，那么能够自托管和定制我们使用的应用程序是维护民主的唯一方式:它将防止数据资本化和人为操纵。

今天的编程就像古埃及时期的写作一样:一种完全原始的工具，但却能有效地收集和保留人类的力量，因为它是原始的。

我们需要更好的系统、更好的编程语言，以及能够在不被操纵的情况下使用软件的人。

在那之前，广泛采用人工智能可能是有用的，但将其应用于人类数据是不负责任的。我们需要谨慎的法规，而不是过于谨慎，不是因为计算机辅助统计本身是危险的，而是因为在大多数人无法理解他们的工作的情况下，太容易滥用它，操纵或伤害人们和社会。