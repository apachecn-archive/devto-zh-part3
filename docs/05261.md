# 书摘要:马克斯·泰格马克的《生活 3.0》

> 原文:[https://dev . to/halldorstefans/book-summary-life-3-0-by-max-TEG mark-5ae 3](https://dev.to/halldorstefans/book-summary-life-3-0-by-max-tegmark-5ae3)

*最初发表于 2019 年 4 月 29 日[halldorstefans.com](https://www.halldorstefans.com/book-summary-life-3-0-by-max-tegmark/)我的博客*

### [](#1-how-do-you-define-life-and-intelligence)1。你如何定义生命和智慧？

这本书把人生分成三个阶段:

*   生命 1.0(生物阶段)-硬件和软件经过几代人的进化
*   生活 2.0(文化阶段)——软件是设计出来的(通过学习)
*   生活 3.0(科技阶段)——硬件也发展。目前不存在。AI 够得着吗？

三个主要的意见分歧是:

*   技术怀疑论者认为建造超人的人工智能(AGI)太难了，几百年内都不会发生。
*   数字乌托邦者认为这有可能在本世纪发生，并对此表示欢迎。
*   有益的人工智能运动也认为这可能在本世纪实现，但不保证会有好的结果，需要继续努力。

我们需要确保每个单词的意思是准确的，并提防常见的误解。
例如:

| 术语 |  |
| --- | --- |
| 生活 | 能够保持其复杂性和可复制性的流程 |
| 智力 | 完成复杂目标的能力 |
| 人工通用智能(AGI) | 完成任何认知任务的能力至少和人类一样好 |
| 超级智能 | 普遍智力远超人类水平 |

一些常见的误解是:

| 神话般的担忧/神话 | 实际的担忧/事实 |
| --- | --- |
| AI 变得邪恶或有意识 | 人工智能变得有能力，与我们的目标不一致 |
| 2100 年的超级智能是不可避免的，也是不可能的 | 它可能会在几十年、几个世纪或者永远不会发生:人工智能专家不同意&我们根本不知道 |

-我们应该以什么样的未来为目标，如何实现？

### [](#2-can-a-material-have-intelligence-and-learn)2。一个材料可以有智能和学习吗？

智力，用我们上面的定义，不能用单一的智商来衡量，只能用一系列的能力来衡量。今天的人工智能是狭义的，每个系统只能完成一个特定的目标，而人类的智能是广义的。例如，用不同的强调说一个句子可以戏剧性地改变句子的意思，这是今天的计算机无法解释的。

记忆、计算和学习对他们来说有一种抽象、无形的感觉，因为他们能够独立生活，不依赖或反映他们底层物质的细节。

#### [](#memory)记忆

任何物质块都可以是内存的物理层，只要它有许多不同的稳定状态。如果我们想象自己是一个有 16 个山谷的鸡蛋床垫。这 16 个山谷中的任何一个都可以代表一段记忆。例如，如果你把一张纸放在 7 号山谷，那么你知道你总是可以去 7 号山谷得到那张纸上的信息。所以我们可以把记忆定义为在一个稳定的位置存储信息的东西。其他示例包括书籍(特定页码上的信息)和硬盘驱动器(将数据存储在磁盘上的特定位置)。

#### [](#computation)计算

任何物质都可以计算，只要它包含某些通用的构建模块，这些模块可以组合起来实现任何功能。与非门和神经元是这种通用“计算原子”的重要例子。NAND 之所以重要，是因为任何真或假的数学函数都可以通过使用 NAND 门的组合来实现。

#### [](#learning)学习

当人类在学习时，我们正在构建、改变和/或连接我们当前的理解，以变得更好。我们正在整理我们的知识，以增加我们的理解。例如，袖珍计算器永远学不到新东西。它们总是以同样的速度和同样的精度给出同样的结果。那是因为人类发明了袖珍计算器。对于一个要学习的事物来说，它必须重新安排自己变得更好。神经网络是学习的坚实基础，因为只要遵守物理定律，它就可以重新排列自己，越来越好地实现所需的计算。

类似于摩尔定律，一旦技术变得两倍强大，它通常可以设计和构建两倍强大的技术。这降低了信息技术的成本，开启了信息时代。

### [](#3-what-can-happen-in-the-near-future)3。在不久的将来会发生什么？

人工智能的近期进展可能会改善我们的法律，让我们的个人生活、电网和金融市场更加高效，并通过自动驾驶汽车、手术机器人和人工智能诊断系统拯救生命。

由 AI 控制的系统需要健壮。解决与验证、确认、安全和控制相关的棘手技术问题。特别是对于人工智能控制的武器系统，风险可能是巨大的。人工智能研究人员和机器人专家呼吁制定一项国际条约，禁止某些种类的自主武器，以避免失控的军备竞赛。

如果我们能弄清楚如何让人工智能变得透明和公正，法律体系会变得更加公平和高效。法律需要更新，因为人工智能提出了涉及隐私、责任和监管的棘手法律问题。

在就业市场上，智能机器正越来越多地取代我们。人工智能创造的财富可以重新分配，让每个人过得更好。否则，经济学家认为不平等将会加剧。低就业社会应该在经济上繁荣，人们从工作之外的活动中获得目标感，并提前规划。

职业建议:进入一个机器不擅长的行业——包括人、不可预测性和创造性。

### [](#4-can-we-control-the-intelligence-explosion)4。我们能控制智能爆炸吗？

建设 AGI 可能会引发智能爆炸，让我们远远落后。如果一群人控制了智能爆炸，他们可能会在几年内接管世界。如果我们不能控制它，AI 可能会接管世界。

拖上几年或几十年的缓慢的情报爆炸，更有可能导致一个多极场景，在大量独立实体之间实现权力平衡。超级智能可能导致“老大哥”控制或更多的个人授权。

-AI 会是人类发生的最好还是最坏的事情？我们更喜欢哪种结果？
-我们如何朝那个方向行驶？

如果我们不知道自己想要什么，我们就不可能得到它。

### [](#5-what-could-happen-the-next-10000-years)5。接下来的一万年会发生什么？

迈向 AGI 的竞赛可能会在多种情况下结束:

*   超级智能与人类和平共处，要么是被迫的，要么是因为它“友好”，它想这样。
*   人工智能或人类会通过忘记技术或缺乏建造它的动机来阻止超级智能。
*   人类灭绝，取而代之的是人工智能或者什么都没有。

-哪种情况是可取的？

我们需要进行这样的对话，这样我们才不会偏离或驶向一个不幸的方向。

### [](#6-what-about-the-next-billion-years-and-beyond)6。下一个十亿年及以后呢？

与宇宙存在相比，智能爆炸是一个突发事件，在宇宙存在中，技术偏离了物理定律的极限。这项技术可以产生 100 亿倍的能量，存储 12-18 个数量级的信息，甚至在一定量的物质下，计算速度可以提高 31-41 倍。超级智能不仅能提高现有资源的效率，还能通过从宇宙中获取资源，使今天的生态系统增长约 32 个数量级。

全球范围内共享或交易的主要资产可能是信息。使用本地警卫 AI 的远程中央枢纽可以鼓励通信中的合作。两种文明冲突可能导致合作或战争。完全有可能的是，我们是唯一有能力让我们的可观测宇宙在未来活过来的生命形式。

为了防止我们灭绝，我们的技术需要改进。如果我们小心翼翼地不断改进技术，生命有可能在地球上繁衍数十亿年。

### [](#7-what-are-our-goals)7。我们的目标是什么？

如前所述，智力是完成复杂目标的能力。人类生命进化的目标是复制。然而，人类不再有像复制这样的简单目标，当我们的感觉与我们基因的目标冲突时，我们会通过使用节育来服从我们的感觉。

我们正在建造越来越多的智能机器来帮助我们实现目标。将机器目标与我们自己的目标结合起来涉及三个未解决的问题；让机器学习它们，采用它们并保留它们。

-我们如何将伦理原则应用于非人类动物和未来的 AI？我们如何给超级智能人工智能设定一个目标，既不会不确定，也不会导致人类的灭绝，从而及时重启对哲学中一些最棘手问题的研究？

### [](#8-what-is-consciousness)8。什么是意识？

在本书中，意识的广义定义是“主观体验”。
意识的三个问题是:

*   “相当困难的问题”——预测哪些物理系统是有意识的。
*   “更难的问题”——预测主观的、有意识的体验。
*   “真正困难的问题”——为什么任何东西都是有意识的。

这个“相当困难的问题”是科学的，因为一个预测你大脑中哪些过程是有意识的理论是可以通过实验来检验和证伪的。根据神经科学实验，许多行为和大脑区域是无意识的，我们的意识体验代表了大量无意识信息的总结。

将意识预测从大脑推广到机器需要一个理论。意识似乎需要一种特别的信息处理过程，这种信息处理过程是相当自主和整合的，因此整个系统是相当自主的，但它的各个部分却不是。

如果意识是信息以某种复杂的方式处理时的感觉，那么重要的只是信息处理的结构，而不是进行信息处理的物质的结构。

如果人工意识是可能的，那么与我们人类所能经历的相比，人工智能经历可能是巨大的。

没有意识就没有意义，因此有意识的存在赋予我们的宇宙意义。

* * *

我非常喜欢这本书。马克斯提出了许多问题，让你思考关于人工智能的未来需要讨论什么。我有时很难理解一些概念，但总的来说，我会把它推荐给任何对人工智能感兴趣的人。

* * *

感谢您的阅读。对于我的每周更新，你可以注册我的[时事通讯。](https://email.halldorstefans.com/)